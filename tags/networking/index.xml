<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>networking on maelvls dev blog</title><link>https://maelvls.dev/tags/networking/</link><description>Recent content in networking on maelvls dev blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 13 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://maelvls.dev/tags/networking/index.xml" rel="self" type="application/rss+xml"/><item><title>How do packets find their way back?</title><link>https://maelvls.dev/how-do-packets-come-back/</link><pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate><guid>https://maelvls.dev/how-do-packets-come-back/</guid><description>In this article, I will expand on the last diagram I drew for my post &amp;ldquo;The Packet&amp;rsquo;s-Eye View of a Kubernetes Service&amp;quot;. Here is the diagram:
Notice one tiny mistake here: the port in src: 1.2.3.4:80 is off! When connecting to a remote host, the TCP stack picks a random ephemeral IP above or equal 32768. The kernel calls it &amp;ldquo;local ports&amp;rdquo;, see inet_hash_connect, secure_ipv4_port_ephemeral and ip_local_port_range. Let us fix this mistake and use the local port 32345 for example.</description></item><item><title>The Packet's-Eye View of a Kubernetes Service</title><link>https://maelvls.dev/packets-eye-kubernetes-service/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://maelvls.dev/packets-eye-kubernetes-service/</guid><description>2. how service and ingress interact with their controllers 3. traffic flow with GKE's service LB and Traefik 4. using my own service controller 5. traffic flow with my own service controller 6. comparison, benchmark, recap -- A few weeks back, I had already written about how to avoid the expensive GKE load balancer. This time, I want to go a bit deeper and detail how the Service object routes packets to a pod and how the &amp;lsquo;hostPort&amp;rsquo; method actually works under the hood.</description></item><item><title>Debugging Kubernetes Networking: my kube-dns is not working!</title><link>https://maelvls.dev/debugging-kubernetes-networking/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://maelvls.dev/debugging-kubernetes-networking/</guid><description>When I scaled my GKE cluster from one node to two nodes, I realised there was some DNS issues with one of the pods on the new Node 2 (that&amp;rsquo;s what I initially thought).
So I went into pod-on-2 (10.24.12.40) and checked that DNS wasn&amp;rsquo;t working. What I did is run
% gcloud compute ssh node-2 % docker run --rm -it --net=container:$(docker ps | grep POD_pod-on-2 | head -1 | cut -f1 -d&amp;#34; &amp;#34;) nicolaka/netshoot % nslookup github.</description></item><item><title>Avoid GKE's expensive load balancer by using hostPort</title><link>https://maelvls.dev/avoid-gke-lb-with-hostport/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>https://maelvls.dev/avoid-gke-lb-with-hostport/</guid><description>I like being able to keep my own GKE Kubernetes cluster for experimenting. But I realized that this Network Load Balancer was way too expensive.
What happens is that GKE has a LoadBalancer controller running (I can&amp;rsquo;t see it) and whenever I have a service with type: LoadBalancer, it will create a L4 load balancer that balances 80 and 443 traffic across the nodes. Here is a diagram of my current setup:</description></item></channel></rss>