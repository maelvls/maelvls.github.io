<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>networking on maelvls dev blog</title>
    <link>https://maelvls.dev/do-not-share-yet/tags/networking/</link>
    <description>Recent content in networking on maelvls.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Jul 2020 19:17:24 +0200</lastBuildDate>
    
    <atom:link href="https://maelvls.dev/do-not-share-yet/tags/networking/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using mitmproxy to understand what kubectl does under the hood</title>
      <link>https://maelvls.dev/do-not-share-yet/mitmproxy-kubectl/</link>
      <pubDate>Wed, 01 Jul 2020 19:17:24 +0200</pubDate>
      
      <guid>https://maelvls.dev/do-not-share-yet/mitmproxy-kubectl/</guid>
      <description>
      
      Mitmproxy is an excellent tool that helps us understand what network calls are made by programs. And kubectl is one of these interesting programs, but it uses a mutual TLS authentication which is tricky to get right.
      
      </description>
    </item>
    <item>
      <title>How do packets find their way back?</title>
      <link>https://maelvls.dev/do-not-share-yet/how-do-packets-come-back/</link>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://maelvls.dev/do-not-share-yet/how-do-packets-come-back/</guid>
      <description>
      
      In one of my previous posts, I studied how traffic flows when using
Kubernetes Services. While drawing the last diagram, I did not clearly
see how traffic could make its way back to the user. In this
post, I focus on how packets find their way back and what makes stateless
rewriting interesting.

      
      </description>
    </item>
    <item>
      <title>The Packet&#39;s-Eye View of a Kubernetes Service</title>
      <link>https://maelvls.dev/do-not-share-yet/packets-eye-kubernetes-service/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://maelvls.dev/do-not-share-yet/packets-eye-kubernetes-service/</guid>
      <description>
      
      The Service and Ingress respectively brings L4 and L7 traffics to your
pods. In this article, I focus on how traffic flows in and what are the
interactions between the ingress controller and the &#34;service-lb controller&#34;
(the thing that creates the external load balancer). I also detail how the
`hostPort` approach shapes traffic.

      
      </description>
    </item>
    <item>
      <title>Debugging Kubernetes Networking: my kube-dns is not working!</title>
      <link>https://maelvls.dev/do-not-share-yet/debugging-kubernetes-networking/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://maelvls.dev/do-not-share-yet/debugging-kubernetes-networking/</guid>
      <description>
      
      Some pods were unable to connect to the kube-proxy pod on one of my GKE
Kubernetes clusters. This post present an in-depth investigation using
tcpdump, wireshark and iptables tracing.

      
      </description>
    </item>
    <item>
      <title>Avoid GKE&#39;s expensive load balancer by using hostPort</title>
      <link>https://maelvls.dev/do-not-share-yet/avoid-gke-lb-with-hostport/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://maelvls.dev/do-not-share-yet/avoid-gke-lb-with-hostport/</guid>
      <description>
      
      I want to avoid using the expensive Google Network Load Balancer and
instead do the load balancing in-cluster using akrobateo, which
acts as a LoadBalancer controller.

      
      </description>
    </item>
    
  </channel>
</rss>
